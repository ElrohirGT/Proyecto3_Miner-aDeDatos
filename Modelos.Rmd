---
title: "Modelos"
author: "Flavio Galán, Gustavo Cruz, Pedro Guzmán"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true # table of content true
    toc_float: true  # upto three depths of headings (specified by #, ## and ###)
    theme: paper  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r Cargar librerias}
library(rio)
library(janitor)
library(ggplot2)
library(dplyr)
library(tidyr)
library(viridis)
library(scales)
library(readxl)
library(purrr)
library(plotly)
library(fastDummies)
library(hopkins)
library(fpc)
library(factoextra)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(NbClust) #Para determinar el número de clusters óptimo
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap)
library(flexclust)
library(ggrepel)
# Dependencias necesarias para el random Forest
# Ahora se usa ranger para randomForest y parallel para paralelizar y que sea mas rapido entrenar
library(randomForest) # al principio se uso randomForest pero era muy lento
library(ranger)
library(parallel)
library(caret)
library(forcats)
library(data.table)
library(tidyverse)
```

# Modelos con Algoritmos

Determinamos que la variable respuesta para nuestra investigación sería el ratio que describimos con anterioridad en el análisis exploratorio entre la edad de la víctima y el agresor. La cual se encuentra definida como:

$$
q=\frac{edad\ victima}{edad\ agresor}
$$

Este nuevo indicador "q" lo dividimos en 3 categorías, según qué tanta diferencia de edad se encontró entre víctimas y agresores. Las categrías son las siguientes:

* Edad similar. El $q\in(0.8,1.2)$.
* Victima mucho menor. El $q \le 0.8$.
* Victima mucho mayor. El $q \ge 1.2$.

## División de los datos

Primero, tenemos que unir todos los datasets de los años anteriores, entonces

```{r Unión de los datos}

original <- import("Datos/2023.sav")

addToDataset <- function(year) {
  data <- import(paste("Datos/", year, ".sav", sep=""))
  # reduced <- data[,colnames(original)]
  original <<- bind_rows(original, data)
}

ifValueConvertToNA <- function(column, values) {
  # print(paste("Removing ignored values from:", column))
  original[,c(column)] <<- ifelse(original[,c(column)] %in% values, NA, original[,c(column)])
}

for (year in 2013:2022) {
  addToDataset(year)
}

ignoredValues <- c(9, 99, 999, 9999)
affectedColumns <- c(
  "VIC_EDAD",
  "TOTAL_HIJOS",
  "NUM_HIJ_HOM",
  "NUM_HIJ_MUJ",
  "VIC_ALFAB",
  "VIC_ESCOLARIDAD",
  "VIC_EST_CIV",
  "VIC_GRUPET",
  "VIC_NACIONAL",
  "VIC_TRABAJA",
  "VIC_OCUP",
  "VIC_DEDICA",
  "VIC_DISC",
  "TIPO_DISCAQ",
  "OTRAS_VICTIMAS",
  "VIC_OTRAS_HOM",
  "VIC_OTRAS_MUJ",
  "VIC_OTRAS_N_OS",
  "VIC_OTRAS_N_AS",
  "HEC_DIA",
  "HEC_MES",
  "HEC_ANO",
  "HEC_DEPTO",
  "HEC_DEPTOMCPIO",
  "HEC_AREA",
  "HEC_RECUR_DENUN",
  "INST_DONDE_DENUNCIO",
  "AGR_EDAD",
  "AGR_ALFAB",
  "AGR_ESCOLARIDAD",
  "AGR_EST_CIV",
  "AGR_GURPET",
  "AGR_NACIONAL",
  "AGR_TRABAJA",
  "AGR_OCUP",
  "AGR_DEDICA",
  "AGRESORES_OTROS_TOTAL",
  "AGR_OTROS_HOM",
  "AGR_OTRAS_MUJ",
  "AGR_OTROS_N_OS",
  "AGR_OTRAS_N_AS",
  "CONDUCENTE",
  "LEY_APLICABLE",
  "ARTICULOVIF1",
  "ARTICULOVIF2",
  "ARTICULOVIF3",
  "ARTICULOVIF4",
  "ARTICULOVCM1",
  "ARTICULOVCM2",
  "ARTICULOVCM3",
  "ARTICULOVCM4",
  "ARTICULOCODPEN1",
  "ARTICULOCODPEN2",
  "ARTICULOCODPEN3",
  "ARTICULOCODPEN4",
  "ARTICULOTRAS1",
  "ARTICULOTRAS2",
  "ARTICULOTRAS3",
  "ARTICULOTRAS4",
  "MEDIDAS_SEGURIDAD",
  "ORGANISMO_REMITE",
  "QUIEN_REPORTA",
  "ORGANISMO_JURISDICCIONAL"
)


for (col in affectedColumns) {
	ifValueConvertToNA(col, ignoredValues)
}

# Ignorar también TIPO_MEDIDA, se ignora con valor z
ifValueConvertToNA("TIPO_MEDIDA", c("z"))

# Por alguna razón se crea esta columna, todos sus valores son NAN así que la borramos.
original$`filter_$` <- NULL

summary(original)
```

Ahora creamos la variable respuesta:
```{r creacion de variable respuesta}
edad_agr_vic <- original %>%
  filter(!is.na(VIC_EDAD) & !is.na(AGR_EDAD)) %>%
  mutate(
    ratio_age = VIC_EDAD / AGR_EDAD,
    diferenciaEdad = ifelse(ratio_age <= 0.8, "Mucho menor", ifelse(ratio_age <= 1.2, "Similar", "Mucho mayor")) 
  )

summary(edad_agr_vic)
```

Con esto podemos decir que el dataset tiene `r ncol(edad_agr_vic)` variables y `r nrow(edad_agr_vic)` observaciones.

Con lo cual decidimos dividir el dataset en 2 grupos, uno de validación y otro para entrenamiento, el de validación tiene el 30% de los datos mientras que el de entrenamiento el 70%. Los grupos se ven así:

```{r División en entrenamiento y validación}
set.seed(69420)
train_index <- createDataPartition(edad_agr_vic$diferenciaEdad, p = 0.7, list = FALSE)
train <- edad_agr_vic[train_index,]
test <- edad_agr_vic[-train_index,]

ggplotly(ggplot(train, aes(x=diferenciaEdad)) +
  geom_bar(fill = "skyblue") +
  labs(title="Datos de Entrenamiento", x = "Categoria", y = "Cuenta"))


ggplotly(ggplot(test, aes(x=diferenciaEdad)) +
  geom_bar(fill = "orange") +
  labs(title = "Datos de Validacion", x = "Categoria", y = "Cuenta"))
```

Como se puede ver existe una alta desigualdad en los datos, puesto que la gran mayoría de los casos se dan cuando la víctima tiene una edad similar al agresor. Definitivamente esto será algo a tomar en cuenta durante el entrenamiento del modelo, uno de las posibles optimizaciones a evaluar podría ser balancear la data de entrenamiento para mejorar la precisión.


## Modelo elegido

Se decidió utilizar el modelo random forest ya qu, a pesar de su alto coste computacional, en entregas anteriores se ha determinado que este algoritmo ofrece una mayor presición a la hora de calificar o predecir la información. Para realizar los modelos se realizarán las siguietes transformaciones al conjunto de datos: 


- Sustituir los valores NA en las variables numéricas con el valor de la mediana. 
- Cambiar los NA en las varibles categóricas a una nueva categoría llamada "Desconocido"
- Crear una nueva varible llamadaratio age la cuál será el cociente entre la edad del agresor y la de la víctima
- Tunear el número de variables y el split de variables en cada uno de los nuevos modelos a genenrar.

```{r Modelos rf}
ncores <- min(detectCores() - 1, 16)  # Limitamos a un máximo práctico

robust_impute <- function(df) {
  df_copy <- copy(df)
  
  for (col in names(df_copy)) {
    if (is.numeric(df_copy[[col]])) {
      na_idx <- which(is.na(df_copy[[col]]))
      if (length(na_idx) > 0) {
        if (all(is.na(df_copy[[col]]))) {
          set(df_copy, na_idx, col, 0)
        } else {
          med_val <- median(df_copy[[col]], na.rm = TRUE)
          set(df_copy, na_idx, col, med_val)
        }
      }
    } 
    else if (is.factor(df_copy[[col]])) {
      na_idx <- which(is.na(df_copy[[col]]))
      if (length(na_idx) > 0) {
        if (all(is.na(df_copy[[col]]))) {
          if (!("Desconocido" %in% levels(df_copy[[col]]))) {
            levels(df_copy[[col]]) <- c(levels(df_copy[[col]]), "Desconocido")
          }
          set(df_copy, na_idx, col, "Desconocido")
        } else {
          freq_table <- table(df_copy[[col]], useNA = "no")
          if (length(freq_table) > 0) {
            most_freq <- names(which.max(freq_table))
            set(df_copy, na_idx, col, most_freq)
          } else {
            if (!("Desconocido" %in% levels(df_copy[[col]]))) {
              levels(df_copy[[col]]) <- c(levels(df_copy[[col]]), "Desconocido")
            }
            set(df_copy, na_idx, col, "Desconocido")
          }
        }
      }
    }
    else if (is.character(df_copy[[col]])) {
      df_copy[[col]] <- as.factor(df_copy[[col]])
      na_idx <- which(is.na(df_copy[[col]]))
      if (length(na_idx) > 0) {
        set(df_copy, na_idx, col, "Desconocido")
      }
    }
    else {
      na_idx <- which(is.na(df_copy[[col]]))
      if (length(na_idx) > 0) {
        df_copy[[col]] <- as.character(df_copy[[col]])
        set(df_copy, na_idx, col, "Desconocido")
        df_copy[[col]] <- as.factor(df_copy[[col]])
      }
    }
  }
  
  if (any(is.na(df_copy))) {
    na_cols <- colnames(df_copy)[colSums(is.na(df_copy)) > 0]
    message("ADVERTENCIA: Eliminando columnas con NAs persistentes: ", paste(na_cols, collapse=", "))
    df_copy <- df_copy[, .SD, .SDcols = setdiff(names(df_copy), na_cols)]
  }
  
  return(df_copy)
}

train_imp <- robust_impute(copy(train))
test_imp <- robust_impute(copy(test))

for (col in names(train_imp)) {
  if (is.factor(train_imp[[col]]) && col %in% names(test_imp)) {
    train_levels <- levels(train_imp[[col]])
    test_levels <- levels(test_imp[[col]])
    
    all_levels <- unique(c(train_levels, test_levels))
    
    train_imp[[col]] <- factor(train_imp[[col]], levels = all_levels)
    test_imp[[col]] <- factor(test_imp[[col]], levels = all_levels)
  }
}

for (col in names(train_imp)) {
  if (is.factor(train_imp[[col]]) && length(levels(train_imp[[col]])) > 20) {
    top_levels <- names(sort(table(train_imp[[col]]), decreasing = TRUE)[1:20])
    
    if (length(top_levels) == 0) {
      train_imp[[col]] <- as.character(train_imp[[col]])
      test_imp[[col]] <- as.character(test_imp[[col]])
      next
    }
    
    train_levels_to_change <- setdiff(levels(train_imp[[col]]), c(top_levels, "Other"))
    if (length(train_levels_to_change) > 0) {
      levels(train_imp[[col]])[match(train_levels_to_change, levels(train_imp[[col]]))] <- "Other"
    }
    
    test_levels_to_change <- setdiff(levels(test_imp[[col]]), c(top_levels, "Other"))
    if (length(test_levels_to_change) > 0) {
      levels(test_imp[[col]])[match(test_levels_to_change, levels(test_imp[[col]]))] <- "Other"
    }
    
    combined_levels <- unique(c(levels(train_imp[[col]]), levels(test_imp[[col]])))
    train_imp[[col]] <- factor(train_imp[[col]], levels = combined_levels)
    test_imp[[col]] <- factor(test_imp[[col]], levels = combined_levels)
  }
}

if (any(is.na(train_imp)) || any(is.na(test_imp))) {
  na_cols_train <- names(train_imp)[colSums(is.na(train_imp)) > 0]
  na_cols_test <- names(test_imp)[colSums(is.na(test_imp)) > 0]
  problem_cols <- unique(c(na_cols_train, na_cols_test))
  
  if (length(problem_cols) > 0) {
    message("ADVERTENCIA: Eliminando columnas problemáticas como último recurso: ", 
           paste(problem_cols, collapse=", "))
    
    if ("ratio_age" %in% problem_cols) {
      stop("La variable objetivo 'ratio_age' contiene NAs que no se pueden eliminar. Revisa tus datos.")
    }
    
    cols_to_keep <- setdiff(names(train_imp), problem_cols)
    train_imp <- train_imp[, ..cols_to_keep]
    test_imp <- test_imp[, ..cols_to_keep]
  }
}

stopifnot("Aún hay NAs en train_imp" = sum(is.na(train_imp)) == 0)
stopifnot("Aún hay NAs en test_imp" = sum(is.na(test_imp)) == 0)

p <- ncol(train_imp) - 1  

# Modelo 1: Menos árboles, menos variables por split (enfoque conservador)
train_model1 <- function() {
  set.seed(123)
  ranger(
    ratio_age ~ ., 
    data = train_imp,
    num.trees = 200,                 # Menos árboles para velocidad
    mtry = floor(sqrt(p)),           # Enfoque tradicional: sqrt(p)
    min.node.size = 15,              # Nodos más grandes (menos sobreajuste)
    max.depth = 15,                  # Profundidad limitada
    sample.fraction = 0.7,           # Submuestreo
    num.threads = ncores,
    verbose = FALSE,                 
    importance = "impurity"          # Más rápido
  )
}

# Modelo 2: Número medio de árboles, más variables por split
train_model2 <- function() {
  set.seed(456)
  ranger(
    ratio_age ~ ., 
    data = train_imp,
    num.trees = 300,                 # Más árboles, mejor generalización
    mtry = floor(p/3),               # Más variables por split
    min.node.size = 5,               # Nodos más pequeños
    max.depth = 25,                  # Mayor profundidad
    sample.fraction = 0.8,           # Más muestras
    num.threads = ncores,
    verbose = FALSE,
    importance = "impurity"
  )
}

# Modelo 3: Más árboles, balance de variables
train_model3 <- function() {
  set.seed(789)
  ranger(
    ratio_age ~ ., 
    data = train_imp,
    num.trees = 500,                 # Muchos árboles
    mtry = floor(p * 0.4),           # Balance de variables
    min.node.size = 10,              # Balance en tamaño de nodos
    max.depth = 20,                  # Balance en profundidad
    sample.fraction = 0.75,          # Balance en submuestreo
    num.threads = ncores,
    verbose = FALSE,
    importance = "impurity"
  )
}

message("Entrenando modelo 1 (Conservador)")
rf1 <- train_model1()
message("Entrenando modelo 2 (Intermedio)")
rf2 <- train_model2()
message("Entrenando modelo 3 (Agresivo)")
rf3 <- train_model3()

pred1 <- predict(rf1, data = test_imp)$predictions
pred2 <- predict(rf2, data = test_imp)$predictions
pred3 <- predict(rf3, data = test_imp)$predictions

calc_metrics <- function(pred, actual) {
  c(
    MAE = mean(abs(pred - actual)),
    RMSE = sqrt(mean((pred - actual)^2)),
    R2 = 1 - sum((actual - pred)^2) / sum((actual - mean(actual))^2)
  )
}

metrics1 <- calc_metrics(pred1, test_imp$ratio_age)
metrics2 <- calc_metrics(pred2, test_imp$ratio_age)
metrics3 <- calc_metrics(pred3, test_imp$ratio_age)

# Comparamos los modelos
all_metrics <- rbind(
  Model1 = metrics1,
  Model2 = metrics2,
  Model3 = metrics3
)

# Graficos para ayudar a entender el rendimiento de los modelos rf creados y entrenados anteriormente
if (require(ggplot2)) {
  metrics_df <- as.data.frame(all_metrics)
  metrics_df$Modelo <- rownames(metrics_df)
  
  # Tabla de métricas
  print("Tabla de métricas de los tres modelos:")
  print(all_metrics)
  
  # Gráfico de barras de RMSE
  ggplot(metrics_df, aes(x = Modelo, y = RMSE, fill = Modelo)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = round(RMSE, 4)), vjust = -0.5) +
    labs(title = "Comparación de RMSE entre modelos",
         y = "RMSE (menor es mejor)") +
    theme_minimal() +
    theme(legend.position = "none")
  
  # Gráficos de predicción vs real para cada modelo
  prediction_df <- data.frame(
    Real = test_imp$ratio_age,
    Modelo1 = pred1,
    Modelo2 = pred2,
    Modelo3 = pred3
  )
  
  # Gráfico para Modelo 1
  plot_pred1 <- ggplot(prediction_df, aes(x = Real, y = Modelo1)) +
    geom_point(alpha = 0.5, color = "blue") +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Predicción vs Valor Real (Modelo 1)",
         x = "Ratio de Edad Real",
         y = "Ratio de Edad Predicho") +
    theme_minimal() +
    annotate("text", x = min(prediction_df$Real), y = max(prediction_df$Modelo1), 
             label = paste("RMSE:", round(metrics1["RMSE"], 4)), hjust = 0)
  
  # Gráfico para Modelo 2
  plot_pred2 <- ggplot(prediction_df, aes(x = Real, y = Modelo2)) +
    geom_point(alpha = 0.5, color = "green") +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Predicción vs Valor Real (Modelo 2)",
         x = "Ratio de Edad Real",
         y = "Ratio de Edad Predicho") +
    theme_minimal() +
    annotate("text", x = min(prediction_df$Real), y = max(prediction_df$Modelo2), 
             label = paste("RMSE:", round(metrics2["RMSE"], 4)), hjust = 0)
  
  # Gráfico para Modelo 3
  plot_pred3 <- ggplot(prediction_df, aes(x = Real, y = Modelo3)) +
    geom_point(alpha = 0.5, color = "purple") +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Predicción vs Valor Real (Modelo 3)",
         x = "Ratio de Edad Real",
         y = "Ratio de Edad Predicho") +
    theme_minimal() +
    annotate("text", x = min(prediction_df$Real), y = max(prediction_df$Modelo3), 
             label = paste("RMSE:", round(metrics3["RMSE"], 4)), hjust = 0)
  
  print(plot_pred1)
  print(plot_pred2)
  print(plot_pred3)
}

```

## Resultados de los modelos

Se exploraron tres modelos de Random Forest con diferentes configuraciones de parámetros para predecir la variable ratio_age. El Modelo 1, cuenta con menos árboles, menos variables por división y nodos más grandes, en comparación del modelo 2 y 3, y mostró un RMSE relativamente alto de 0.2712 y un R2 de 0.8307. El gráfico de dispersión revela una mayor dispersión de los puntos alrededor de la línea de predicción perfecta, indicando una menor precisión en las predicciones. En contraste, el Modelo 2, que empleó un mayor número de árboles, más variables por división y nodos más pequeños, demostró una mejora significativa en el rendimiento, con un RMSE de 0.096 y un R2 de 0.9788. El gráfico correspondiente exhibe una concentración mucho mayor de los puntos cerca de la línea de predicción ideal, lo que sugiere predicciones más precisas. Finalmente, el Modelo 3, que buscó un equilibrio en el número de árboles, la cantidad de variables por división y el tamaño de los nodos, logró los mejores resultados con un RMSE de 0.0824 y un R2 de 0.9844. Su gráfico de dispersión muestra la menor dispersión de los puntos, indicando la mayor exactitud predictiva entre los tres modelos.

La elección de estos parámetros se basó en la búsqueda de un equilibrio entre la capacidad del modelo para capturar patrones complejos en los datos y la necesidad de evitar el sobreajuste. El Modelo 1, al ser más conservador, probablemente incurrió en un mayor sesgo al no permitir suficiente flexibilidad en el aprendizaje. Los Modelos 2 y 3, al aumentar la complejidad, más árboles y más variables consideradas en cada división, lograron reducir significativamente el error. 

```{r Modelo final}

# Determinamos el mejor modelo basado en RMSE
best_idx <- which.min(all_metrics[, "RMSE"])
best_model_name <- paste("Modelo", best_idx)

message("El mejor modelo es ", best_model_name, " con RMSE = ", all_metrics[best_idx, "RMSE"])

var_importance <- switch(best_idx,
  "1" = importance(rf1),
  "2" = importance(rf2),
  "3" = importance(rf3)
)

var_importance <- sort(var_importance, decreasing = TRUE)

message("Entrenando modelo FINAL con parámetros optimizados")
best_params <- switch(best_idx,
  "1" = list(trees = 200, mtry = floor(sqrt(p)), node_size = 15, depth = 15, fraction = 0.7),
  "2" = list(trees = 500, mtry = floor(p/3), node_size = 5, depth = 25, fraction = 0.8),
  "3" = list(trees = 800, mtry = floor(p * 0.4), node_size = 10, depth = 20, fraction = 0.75)
)

# Creamos un modelo final con los parámetros optimizados
set.seed(2077)
rf_final <- ranger(
  ratio_age ~ ., 
  data = train_imp,
  num.trees = best_params$trees + 100,    # Aumentamos ligeramente los árboles
  mtry = best_params$mtry,                # Mantenemos la mejor configuración de variables
  min.node.size = best_params$node_size,  # Mantenemos el mejor tamaño de nodo
  max.depth = best_params$depth,          # Mantenemos la mejor profundidad
  sample.fraction = best_params$fraction, # Mantenemos la mejor fracción de muestreo
  num.threads = ncores,
  verbose = FALSE,
  importance = "impurity"
)

pred_final <- predict(rf_final, data = test_imp)$predictions
final_metrics <- c(
  MAE = mean(abs(pred_final - test_imp$ratio_age)),
  RMSE = sqrt(mean((pred_final - test_imp$ratio_age)^2)),
  R2 = 1 - sum((test_imp$ratio_age - pred_final)^2) / 
       sum((test_imp$ratio_age - mean(test_imp$ratio_age))^2)
)

message("Resultados del modelo FINAL:")
print(final_metrics)

if (require(ggplot2)) {
  # Tabla comparativa: mejor modelo inicial vs modelo final
  best_model_metrics <- all_metrics[best_idx,]
  comparison_metrics <- rbind(
    "Mejor Modelo Inicial" = best_model_metrics,
    "Modelo Final Optimizado" = final_metrics
  )
  print("Comparación entre el mejor modelo inicial y el modelo final:")
  print(comparison_metrics)
  
  # Gráfico mejorado de predicción vs real para el modelo final
  prediction_final_df <- data.frame(
    Real = test_imp$ratio_age,
    Predicho = pred_final
  )
  
  # Añadimos línea de regresión y error
  plot_final <- ggplot(prediction_final_df, aes(x = Real, y = Predicho)) +
    geom_point(alpha = 0.5, color = "darkgreen") +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    geom_smooth(method = "lm", color = "blue", se = TRUE) +
    labs(title = "Predicción vs Valor Real (Modelo FINAL)",
         subtitle = paste("RMSE:", round(final_metrics["RMSE"], 4), 
                          "| R²:", round(final_metrics["R2"], 4)),
         x = "Ratio de Edad Real",
         y = "Ratio de Edad Predicho") +
    theme_minimal() +
    coord_fixed(ratio = 1) +
    theme(plot.title = element_text(face = "bold"),
          plot.subtitle = element_text(color = "darkblue"))

  print(plot_final)
}

# Gráfico de predicción vs real
if (require(ggplot2)) {
  prediction_df <- data.frame(
    Real = test_imp$ratio_age,
    Predicho = pred_final
  )
  
  plot_pred <- ggplot(prediction_df, aes(x = Real, y = Predicho)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Predicción vs Valor Real",
         x = "Ratio de Edad Real",
         y = "Ratio de Edad Predicho") +
    theme_minimal()
  
  print(plot_pred)
}

message("Resumen de importancia de variables (top 5):")
print(head(var_importance, 5))
message("Análisis completado. El modelo final ha sido guardado como 'rf_model_final.rds'")
```

Debido a los resultados obtenidos de los 3 modelos iniciales, el Modelo 3 se eligió como el de mejor rendimiento en el conjunto de prueba, con un RMSE de 0.0824 y un R2 de 0.9844. Basándonos en la configuración de hiperparámetros de este modelo, se entrenó un Modelo Final Optimizado, realizando un ligero ajuste al aumentar el número de árboles a 600. La evaluación de este modelo final en el conjunto de prueba resultó en una ligera mejora en las métricas de rendimiento, alcanzando un MAE de 0.0120, un RMSE de 0.0821 y un R2 de 0.9845. 

El gráfico de dispersión del modelo final muestra una concentración aún mayor de los puntos alrededor de la línea de predicción perfecta, lo que indica una alta precisión en las predicciones. 
No obstante, la mejora marginal también indica que el modelo inicial ya se encontraba muy cerca de su máximo potencial con los datos disponibles. En cuanto al sobreajuste, la consistencia en el rendimiento entre el mejor modelo inicial y el modelo final en el conjunto de prueba, incluso con un ligero aumento en la complejidad al añadir más árboles, sugiere que el modelo final generaliza bien a datos no vistos y no presenta signos evidentes de sobreajuste. El alto R2 indica que una gran proporción de la varianza en la variable objetivo es explicada por el modelo, mientras que el bajo RMSE señala que las diferencias entre los valores predichos y reales son pequeñas.

